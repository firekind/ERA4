seed_everything: 0
model:
  vocab_size: 49152
  hidden_size: 576
  num_hidden_layers: 30
  num_attention_heads: 9
  num_key_value_heads: 3
  intermediate_size: 1536
  rms_norm_eps: 1e-5
  max_position_embeddings: 2048
  rope_theta: 100000.0
  tie_word_embeddings: True
  learning_rate: 3e-4
  weight_decay: 0.01
  warmup_steps: 2000
  max_steps: 5050
data:
  file_path: data/dataset/input-1.txt
  batch_size: 8
  num_workers: 4
  seq_length: 256
trainer:
  max_steps: 5050
  precision: bf16-mixed
  gradient_clip_val: 1.0
  log_every_n_steps: 10
  default_root_dir: data/logs
  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        filename: "smollm2-{step:05d}-{train.loss:.4f}"
        monitor: train.loss
        mode: min
        save_top_k: 3
        save_last: true
        verbose: true
    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step
    - class_path: lightning.pytorch.callbacks.TQDMProgressBar
      init_args:
        leave: true
    - class_path: session_13.TextGenerationCallback
      init_args:
        max_length: 50
        every_n_steps: 500
