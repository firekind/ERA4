seed_everything: 0
model:
  vocab_size: 49152
  hidden_size: 768
  num_hidden_layers: 30
  num_attention_heads: 8
  compression_ratio: 8
  intermediate_size: 1536
  num_routed_experts: 7
  num_shared_experts: 1
  num_experts_per_tok: 2
  rms_norm_eps: 1e-5
  max_position_embeddings: 2048
  rope_theta: 10000.0
  tie_word_embeddings: True
  learning_rate: 1e-4
  weight_decay: 0.01
  warmup_pct: 0.1
data:
  file_path: data/dataset/input-1.txt
  batch_size: 4
  num_workers: 4
  seq_length: 256
trainer:
  max_steps: 11000
  precision: bf16-mixed
  gradient_clip_val: 1.0
  log_every_n_steps: 10
  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        filename: "smoldeepseek-{step:05d}-{train.loss:.4f}"
        monitor: train.loss
        mode: min
        every_n_train_steps: 1000
        save_top_k: 3
        save_last: true
        verbose: true
    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step
    - class_path: lightning.pytorch.callbacks.TQDMProgressBar
      init_args:
        leave: true
  logger:
    class_path: lightning.pytorch.loggers.TensorBoardLogger
    init_args:
      save_dir: data/logs
      name: smoldeepseek
